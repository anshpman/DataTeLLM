{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2879186,"sourceType":"datasetVersion","datasetId":826163}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-11T14:06:22.151432Z","iopub.execute_input":"2025-04-11T14:06:22.151899Z","iopub.status.idle":"2025-04-11T14:06:22.632047Z","shell.execute_reply.started":"2025-04-11T14:06:22.151866Z","shell.execute_reply":"2025-04-11T14:06:22.630665Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/test-file/tested.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"________________________________________________________","metadata":{}},{"cell_type":"code","source":"# Cell 0: Setup\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport io\nimport base64\nimport gc\nimport google.generativeai as genai\nimport pickle # <-- For saving/loading results\nfrom kaggle_secrets import UserSecretsClient\nfrom IPython.display import HTML, display, FileLink\n\nprint(\"--- Running Setup ---\")\n\n# --- Configuration ---\nDATASET_PATH = '/kaggle/input/test-file/tested.csv' # Your dataset path\nOUTPUT_DIR = '/kaggle/working/output'\nRESULTS_FILE = os.path.join(OUTPUT_DIR, 'analysis_results.pkl') # File to save state\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nprint(f\"Output directory: {OUTPUT_DIR}\")\nprint(f\"Results file: {RESULTS_FILE}\")\n\n# --- Load API Key ---\nAPI_SECRET_LABEL = \"GOOGLE_API\" # Your secret label\ngemini_ready = False\ntry:\n    user_secrets = UserSecretsClient()\n    GEMINI_API_KEY = user_secrets.get_secret(API_SECRET_LABEL)\n    genai.configure(api_key=GEMINI_API_KEY)\n    print(\"Gemini client configured successfully.\")\n    gemini_ready = True\nexcept Exception as e:\n    print(f\"WARNING: Error loading/configuring API Key '{API_SECRET_LABEL}': {e}. LLM features disabled.\")\n\n# --- Load Data ---\ndf = None\ntry:\n    df = pd.read_csv(DATASET_PATH)\n    print(f\"Dataset loaded successfully from: {DATASET_PATH}\")\n    print(f\"Dataset shape: {df.shape}\")\nexcept Exception as e:\n    print(f\"ERROR: Failed to load dataset: {e}\")\n\n# --- Initialize and Save Initial Results ---\nanalysis_results = {}\nif df is not None:\n    analysis_results['dataset_path'] = DATASET_PATH\n    analysis_results['output_dir'] = OUTPUT_DIR\n    analysis_results['shape'] = df.shape\n    analysis_results['gemini_configured'] = gemini_ready\n    analysis_results['head'] = df.head() # Store head/tail early\n    analysis_results['tail'] = df.tail()\n    # Initialize dictionaries needed later\n    analysis_results['plot_paths'] = {}\n    analysis_results['narratives'] = {}\n    analysis_results['numerical_skewness'] = {}\n    analysis_results['anomaly_results'] = {}\n    analysis_results['clustering_results'] = {}\n    analysis_results['time_analysis_skipped'] = True # Default\n    analysis_results['time_analysis_reason'] = \"Analysis not attempted.\" # Default\n\n    try:\n        with open(RESULTS_FILE, 'wb') as f:\n            pickle.dump(analysis_results, f)\n        print(f\"Initial analysis results saved to {RESULTS_FILE}\")\n    except Exception as e:\n        print(f\"ERROR: Failed to save initial results: {e}\")\nelse:\n    print(\"ERROR: Cannot proceed without loaded DataFrame.\")\n\nprint(\"\\n--- Setup Complete ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T17:23:29.182912Z","iopub.execute_input":"2025-04-11T17:23:29.183313Z","iopub.status.idle":"2025-04-11T17:23:29.392335Z","shell.execute_reply.started":"2025-04-11T17:23:29.183285Z","shell.execute_reply":"2025-04-11T17:23:29.391112Z"}},"outputs":[{"name":"stdout","text":"--- Running Setup ---\nOutput directory: /kaggle/working/output\nResults file: /kaggle/working/output/analysis_results.pkl\nGemini client configured successfully.\nDataset loaded successfully from: /kaggle/input/test-file/tested.csv\nDataset shape: (418, 12)\nInitial analysis results saved to /kaggle/working/output/analysis_results.pkl\n\n--- Setup Complete ---\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Cell 1: Core Analysis\n\nimport pandas as pd\nimport numpy as np\nimport os\nimport pickle\nimport io\n\nprint(\"--- Running Core Analysis ---\")\n\n# --- Load Previous Results ---\nRESULTS_FILE = '/kaggle/working/output/analysis_results.pkl' # Define again or get from results\nanalysis_results = None\nif os.path.exists(RESULTS_FILE):\n    try:\n        with open(RESULTS_FILE, 'rb') as f:\n            analysis_results = pickle.load(f)\n        print(f\"Loaded previous results from {RESULTS_FILE}\")\n    except Exception as e:\n        print(f\"ERROR: Failed to load results file '{RESULTS_FILE}': {e}. Cannot proceed.\")\n        # exit() # Or handle error\nelse:\n    print(f\"ERROR: Results file '{RESULTS_FILE}' not found. Run Setup cell first.\")\n    # exit() # Or handle error\n\n# Ensure df is loaded (it should be in memory if setup ran in same session,\n# but robustly you might pass df path via results and reload here)\n# For now, assume 'df' exists from the setup cell if analysis_results loaded.\nif analysis_results and 'df' in locals() and df is not None:\n    try:\n        print(\"Performing core analysis calculations...\")\n        # Buffer for df.info()\n        buffer = io.StringIO()\n        df.info(buf=buffer)\n        analysis_results['df_info'] = buffer.getvalue()\n\n        # Describe\n        analysis_results['summary_stats'] = df.describe(include='all')\n        print(\" - Calculated summary stats.\")\n\n        # Missing Values\n        missing_values_all = df.isnull().sum()\n        analysis_results['missing_values'] = missing_values_all[missing_values_all > 0]\n        print(f\" - Found missing values in columns: {analysis_results['missing_values'].index.tolist() if not analysis_results['missing_values'].empty else 'None'}\")\n\n        # Unique Counts\n        analysis_results['unique_counts'] = df.nunique()\n        print(\" - Calculated unique counts.\")\n\n        # --- Save Updated Results ---\n        try:\n            with open(RESULTS_FILE, 'wb') as f:\n                pickle.dump(analysis_results, f)\n            print(f\"Core analysis results saved to {RESULTS_FILE}\")\n            print(\"\\n--- Core Analysis Complete ---\")\n            print(\"Proceed to next cell (Visualizations).\")\n        except Exception as e:\n            print(f\"ERROR: Failed to save core analysis results: {e}\")\n\n    except Exception as e:\n        print(f\"ERROR during Core Analysis processing: {e}\")\nelse:\n     if not analysis_results:\n         print(\"Cannot run Core Analysis: analysis_results failed to load.\")\n     else:\n         print(\"Cannot run Core Analysis: DataFrame 'df' not found in memory.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T17:23:37.614663Z","iopub.execute_input":"2025-04-11T17:23:37.615105Z","iopub.status.idle":"2025-04-11T17:23:37.691408Z","shell.execute_reply.started":"2025-04-11T17:23:37.615075Z","shell.execute_reply":"2025-04-11T17:23:37.690161Z"}},"outputs":[{"name":"stdout","text":"--- Running Core Analysis ---\nLoaded previous results from /kaggle/working/output/analysis_results.pkl\nPerforming core analysis calculations...\n - Calculated summary stats.\n - Found missing values in columns: ['Age', 'Fare', 'Cabin']\n - Calculated unique counts.\nCore analysis results saved to /kaggle/working/output/analysis_results.pkl\n\n--- Core Analysis Complete ---\nProceed to next cell (Visualizations).\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Cell 2: Basic Visualizations\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport pickle\nimport gc\n\nprint(\"--- Running Basic Visualizations ---\")\n\n# --- Configuration ---\n# Ensure these match the paths used in previous cells\nOUTPUT_DIR = '/kaggle/working/output'\nRESULTS_FILE = os.path.join(OUTPUT_DIR, 'analysis_results.pkl')\n\n# --- Load Previous Results ---\nanalysis_results = None\nif os.path.exists(RESULTS_FILE):\n    try:\n        with open(RESULTS_FILE, 'rb') as f:\n            analysis_results = pickle.load(f)\n        print(f\"Loaded previous results from {RESULTS_FILE}\")\n    except Exception as e:\n        print(f\"ERROR: Failed to load results file '{RESULTS_FILE}': {e}. Cannot proceed.\")\n        # exit() or handle\nelse:\n    print(f\"ERROR: Results file '{RESULTS_FILE}' not found. Run previous cells first.\")\n    # exit() or handle\n\n# --- Check if DataFrame 'df' exists (essential) ---\n# If df wasn't saved/passed, this check fails. Robust way is to load df from path in results.\n# For now, assume 'df' is still in memory from Cell 0 if results loaded.\nif 'df' not in locals() or df is None:\n     print(\"ERROR: DataFrame 'df' not found in memory. Please re-run Cell 0 (Setup).\")\n     # exit() or handle\nelif analysis_results: # Proceed only if results were loaded\n\n    # --- Initialize dicts if they don't exist (safety check) ---\n    analysis_results.setdefault('plot_paths', {}).setdefault('histograms', {})\n    analysis_results['plot_paths'].setdefault('count_plots', {})\n    analysis_results.setdefault('numerical_skewness', {})\n\n    try: # Wrap main logic\n        # --- Numerical Histograms & Skewness ---\n        numerical_cols = df.select_dtypes(include=np.number).columns.tolist()\n        numerical_cols_to_plot = [col for col in numerical_cols if col not in ['PassengerId', 'Survived', 'Pclass']]\n        print(f\"Numerical columns for histograms: {numerical_cols_to_plot}\")\n\n        for col in numerical_cols_to_plot:\n            try:\n                fig, ax = plt.subplots(figsize=(8, 4))\n                sns.histplot(df[col].dropna(), kde=True, ax=ax)\n                ax.set_title(f'Distribution of {col}')\n                plot_filename = os.path.join(OUTPUT_DIR, f'hist_{col}.png')\n                fig.savefig(plot_filename)\n                plt.close(fig) # Close the specific figure\n\n                if os.path.exists(plot_filename):\n                    # Store relative path if desired, or keep absolute\n                    analysis_results['plot_paths']['histograms'][col] = plot_filename\n                else:\n                     print(f\" - FAILED to save hist for {col}\")\n\n                # Calculate skewness\n                analysis_results['numerical_skewness'][col] = df[col].skew()\n            except Exception as e:\n                print(f\" - Error plotting/saving hist for {col}: {e}\")\n\n        # --- Categorical Count Plots ---\n        categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n        potential_numeric_cats = ['Survived', 'Pclass']\n        for col_p in potential_numeric_cats:\n             if col_p in df.columns and col_p not in categorical_cols: categorical_cols.append(col_p)\n\n        # Get unique counts from loaded results\n        unique_counts = analysis_results.get('unique_counts')\n        if unique_counts is None:\n             print(\"WARNING: Unique counts not found in loaded results. Calculating again.\")\n             unique_counts = df.nunique() # Recalculate if missing\n\n        MAX_CATEGORIES_FOR_PLOT = 25\n        categorical_cols_to_plot = sorted(list(set([c for c in categorical_cols if c in unique_counts and unique_counts[c] <= MAX_CATEGORIES_FOR_PLOT])))\n        print(f\"Categorical columns for count plots: {categorical_cols_to_plot}\")\n\n        for col_c in categorical_cols_to_plot:\n            if col_c not in df.columns: continue\n            try:\n                fig, ax = plt.subplots(figsize=(8, 4))\n                sns.countplot(data=df, x=col_c, order=df[col_c].value_counts().index, ax=ax)\n                ax.set_title(f'Count Plot for {col_c}')\n                if len(df[col_c].unique()) > 5:\n                    ax.tick_params(axis='x', rotation=45)\n                fig.tight_layout()\n                plot_filename = os.path.join(OUTPUT_DIR, f'count_{col_c}.png')\n                fig.savefig(plot_filename)\n                plt.close(fig)\n                if os.path.exists(plot_filename):\n                     analysis_results['plot_paths']['count_plots'][col_c] = plot_filename\n            except Exception as e:\n                 print(f\" - Error plotting/saving count plot for {col_c}: {e}\")\n\n        print(\"Basic visualizations successful.\")\n\n        # --- Save Updated Results ---\n        try:\n            with open(RESULTS_FILE, 'wb') as f:\n                pickle.dump(analysis_results, f)\n            print(f\"Visualization results saved to {RESULTS_FILE}\")\n            print(\"\\n--- Basic Visualizations Complete ---\")\n            print(\"Proceed to next cell (Correlation Analysis).\")\n        except Exception as e:\n            print(f\"ERROR: Failed to save visualization results: {e}\")\n\n    except Exception as e:\n        print(f\"ERROR during Basic Visualizations processing: {e}\")\n\n    gc.collect() # Garbage collect","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T17:24:21.605980Z","iopub.execute_input":"2025-04-11T17:24:21.606379Z","iopub.status.idle":"2025-04-11T17:24:23.095301Z","shell.execute_reply.started":"2025-04-11T17:24:21.606345Z","shell.execute_reply":"2025-04-11T17:24:23.094141Z"}},"outputs":[{"name":"stdout","text":"--- Running Basic Visualizations ---\nLoaded previous results from /kaggle/working/output/analysis_results.pkl\nNumerical columns for histograms: ['Age', 'SibSp', 'Parch', 'Fare']\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n","output_type":"stream"},{"name":"stdout","text":"Categorical columns for count plots: ['Embarked', 'Pclass', 'Sex', 'Survived']\nBasic visualizations successful.\nVisualization results saved to /kaggle/working/output/analysis_results.pkl\n\n--- Basic Visualizations Complete ---\nProceed to next cell (Correlation Analysis).\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Cell 3: Correlation Analysis\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport pickle\nimport gc\n\nprint(\"--- Running Correlation Analysis ---\")\n\n# --- Configuration ---\nOUTPUT_DIR = '/kaggle/working/output'\nRESULTS_FILE = os.path.join(OUTPUT_DIR, 'analysis_results.pkl')\n\n# --- Load Previous Results ---\nanalysis_results = None\nif os.path.exists(RESULTS_FILE):\n    try:\n        with open(RESULTS_FILE, 'rb') as f:\n            analysis_results = pickle.load(f)\n        print(f\"Loaded previous results from {RESULTS_FILE}\")\n    except Exception as e:\n        print(f\"ERROR: Failed to load results file '{RESULTS_FILE}': {e}. Cannot proceed.\")\n        # exit() or handle\nelse:\n    print(f\"ERROR: Results file '{RESULTS_FILE}' not found. Run previous cells first.\")\n    # exit() or handle\n\n# --- Check if DataFrame 'df' exists ---\nif 'df' not in locals() or df is None:\n     print(\"ERROR: DataFrame 'df' not found in memory. Please re-run Cell 0 (Setup).\")\n     # exit() or handle\nelif analysis_results: # Proceed only if results were loaded\n\n    # --- Initialize keys ---\n    analysis_results['correlation_skipped'] = True\n    analysis_results.setdefault('plot_paths', {}) # Ensure plot_paths exists\n    analysis_results['highly_correlated_pairs'] = []\n\n    try: # Wrap main logic\n        # --- Select Numerical Columns ---\n        # Exclude IDs, and optionally discrete categoricals like Survived/Pclass\n        cols_for_corr = [col for col in df.select_dtypes(include=np.number).columns.tolist() if col not in ['PassengerId', 'Survived']]\n\n        if len(cols_for_corr) >= 2:\n            print(f\"Calculating correlation matrix for: {cols_for_corr}\")\n\n            # --- Calculate Correlation ---\n            correlation_matrix = df[cols_for_corr].corr()\n            analysis_results['correlation_matrix'] = correlation_matrix # Store matrix if needed later\n            analysis_results['correlation_skipped'] = False\n\n            # --- Generate Heatmap ---\n            try:\n                fig, ax = plt.subplots(figsize=(10, 8))\n                sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5, ax=ax)\n                ax.set_title('Correlation Matrix')\n                fig.tight_layout()\n                plot_filename = os.path.join(OUTPUT_DIR, 'correlation_heatmap.png')\n                fig.savefig(plot_filename)\n                plt.close(fig) # Close figure\n\n                if os.path.exists(plot_filename):\n                    analysis_results['plot_paths']['heatmap'] = plot_filename\n                    print(\" - Saved correlation heatmap.\")\n                else:\n                     print(\" - FAILED to save correlation heatmap.\")\n            except Exception as e:\n                 print(f\" - Error generating/saving heatmap: {e}\")\n\n            # --- Identify Highly Correlated Pairs ---\n            CORR_THRESHOLD = 0.4\n            print(f\" - Identifying pairs with absolute correlation > {CORR_THRESHOLD}\")\n            # Unstack matrix, remove self-correlation, filter by threshold\n            corr_unstacked = correlation_matrix.unstack()\n            strong_pairs_series = corr_unstacked[(abs(corr_unstacked) > CORR_THRESHOLD) & (corr_unstacked != 1.0)]\n\n            highly_corr_pairs_list = []\n            seen_pairs = set()\n            for (var1, var2), corr_value in strong_pairs_series.items():\n                pair = tuple(sorted((var1, var2))) # Ensure ('A','B') is same as ('B','A')\n                if pair not in seen_pairs:\n                    highly_corr_pairs_list.append(((var1, var2), corr_value))\n                    seen_pairs.add(pair)\n\n            analysis_results['highly_correlated_pairs'] = highly_corr_pairs_list\n            if highly_corr_pairs_list:\n                 print(f\" - Found pairs: {highly_corr_pairs_list}\")\n            else:\n                 print(\" - No pairs found above threshold.\")\n\n        else:\n            print(\"Skipping correlation analysis: Need at least 2 numerical columns.\")\n            analysis_results['correlation_skipped'] = True\n\n        print(\"Correlation analysis successful.\")\n\n        # --- Save Updated Results ---\n        try:\n            with open(RESULTS_FILE, 'wb') as f:\n                pickle.dump(analysis_results, f)\n            print(f\"Correlation results saved to {RESULTS_FILE}\")\n            print(\"\\n--- Correlation Analysis Complete ---\")\n            print(\"Proceed to next cell (Anomaly Detection).\")\n        except Exception as e:\n            print(f\"ERROR: Failed to save correlation results: {e}\")\n\n    except Exception as e:\n        print(f\"ERROR during Correlation Analysis processing: {e}\")\n\n    gc.collect() # Garbage collect","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T17:25:23.561123Z","iopub.execute_input":"2025-04-11T17:25:23.561587Z","iopub.status.idle":"2025-04-11T17:25:24.078337Z","shell.execute_reply.started":"2025-04-11T17:25:23.561557Z","shell.execute_reply":"2025-04-11T17:25:24.077212Z"}},"outputs":[{"name":"stdout","text":"--- Running Correlation Analysis ---\nLoaded previous results from /kaggle/working/output/analysis_results.pkl\nCalculating correlation matrix for: ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\n - Saved correlation heatmap.\n - Identifying pairs with absolute correlation > 0.4\n - Found pairs: [(('Pclass', 'Age'), -0.4921427625708605), (('Pclass', 'Fare'), -0.5771473123362406)]\nCorrelation analysis successful.\nCorrelation results saved to /kaggle/working/output/analysis_results.pkl\n\n--- Correlation Analysis Complete ---\nProceed to next cell (Anomaly Detection).\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Cell 4: Anomaly Detection\n\nimport pandas as pd\nimport numpy as np\nimport os\nimport pickle\nimport gc\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.impute import SimpleImputer\n\nprint(\"--- Running Anomaly Detection (Isolation Forest) ---\")\n\n# --- Configuration ---\nOUTPUT_DIR = '/kaggle/working/output'\nRESULTS_FILE = os.path.join(OUTPUT_DIR, 'analysis_results.pkl')\n\n# --- Load Previous Results ---\nanalysis_results = None\nif os.path.exists(RESULTS_FILE):\n    try:\n        with open(RESULTS_FILE, 'rb') as f:\n            analysis_results = pickle.load(f)\n        print(f\"Loaded previous results from {RESULTS_FILE}\")\n    except Exception as e:\n        print(f\"ERROR: Failed to load results file '{RESULTS_FILE}': {e}. Cannot proceed.\")\n        # exit() or handle\nelse:\n    print(f\"ERROR: Results file '{RESULTS_FILE}' not found. Run previous cells first.\")\n    # exit() or handle\n\n# --- Check if DataFrame 'df' exists ---\nif 'df' not in locals() or df is None:\n     print(\"ERROR: DataFrame 'df' not found in memory. Please re-run Cell 0 (Setup).\")\n     # exit() or handle\nelif analysis_results: # Proceed only if results were loaded\n\n    # --- Initialize keys ---\n    analysis_results['anomaly_skipped'] = True\n    analysis_results.setdefault('anomaly_results', {}) # Ensure nested dict exists\n    anomaly_features = ['Age', 'SibSp', 'Parch', 'Fare'] # Features for anomaly detection\n    analysis_results['anomaly_results']['features_used'] = anomaly_features\n\n    try: # Wrap main logic\n        # Select data and handle potential missing columns\n        available_features = [f for f in anomaly_features if f in df.columns]\n        if len(available_features) < 1:\n             print(\"Skipping anomaly detection: None of the specified features exist in the DataFrame.\")\n        else:\n            print(f\"Using features for anomaly detection: {available_features}\")\n            df_anomaly = df[available_features].copy()\n\n            # Check if data is purely NaN before imputing\n            if not df_anomaly.empty and df_anomaly.notna().any().any():\n                # Impute missing values FOR THE MODEL ONLY\n                imputer_ad = SimpleImputer(strategy='median')\n                df_anomaly_imputed = imputer_ad.fit_transform(df_anomaly)\n                # Keep as numpy array for IsolationForest, avoid DataFrame conversion if not needed\n                # df_anomaly_imputed_df = pd.DataFrame(df_anomaly_imputed, columns=available_features, index=df_anomaly.index)\n\n                imputed_cols_msg = [f\"{col}({imputer_ad.statistics_[available_features.index(col)]:.2f})\" for col in available_features if df_anomaly[col].isnull().any()]\n                if imputed_cols_msg:\n                     print(f\" - Imputed NaNs for anomaly detection in: {', '.join(imputed_cols_msg)}.\")\n\n                # Fit Isolation Forest\n                iso_forest = IsolationForest(contamination='auto', random_state=42)\n                predictions = iso_forest.fit_predict(df_anomaly_imputed) # Use imputed numpy array\n\n                # Store results\n                analysis_results['anomaly_results']['predictions'] = predictions # Optional: store predictions\n                analysis_results['anomaly_results']['count'] = np.sum(predictions == -1)\n                analysis_results['anomaly_results']['percentage'] = (np.mean(predictions == -1) * 100)\n                analysis_results['anomaly_skipped'] = False\n\n                print(f\"Anomaly detection successful.\")\n                print(f\" - Found {analysis_results['anomaly_results']['count']} potential anomalies ({analysis_results['anomaly_results']['percentage']:.2f}%).\")\n                # del df_anomaly_imputed_df # Clean up if created\n            else:\n                print(\"Skipping anomaly detection: No suitable features or only NaN values after selection.\")\n\n    except Exception as e:\n        print(f\"ERROR during Anomaly Detection processing: {e}\")\n\n    # --- Save Updated Results ---\n    if analysis_results: # Ensure results exist before saving\n        try:\n            with open(RESULTS_FILE, 'wb') as f:\n                pickle.dump(analysis_results, f)\n            print(f\"Anomaly detection results saved to {RESULTS_FILE}\")\n            print(\"\\n--- Anomaly Detection Complete ---\")\n            print(\"Proceed to next cell (Clustering Exploration).\")\n        except Exception as e:\n            print(f\"ERROR: Failed to save anomaly detection results: {e}\")\n\n    gc.collect() # Garbage collect","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T17:26:37.402372Z","iopub.execute_input":"2025-04-11T17:26:37.402794Z","iopub.status.idle":"2025-04-11T17:26:37.830418Z","shell.execute_reply.started":"2025-04-11T17:26:37.402765Z","shell.execute_reply":"2025-04-11T17:26:37.829279Z"}},"outputs":[{"name":"stdout","text":"--- Running Anomaly Detection (Isolation Forest) ---\nLoaded previous results from /kaggle/working/output/analysis_results.pkl\nUsing features for anomaly detection: ['Age', 'SibSp', 'Parch', 'Fare']\n - Imputed NaNs for anomaly detection in: Age(27.00), Fare(14.45).\nAnomaly detection successful.\n - Found 60 potential anomalies (14.35%).\nAnomaly detection results saved to /kaggle/working/output/analysis_results.pkl\n\n--- Anomaly Detection Complete ---\nProceed to next cell (Clustering Exploration).\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Cell 5: Clustering Exploration\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport pickle\nimport gc\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.decomposition import PCA\n\nprint(\"--- Running Clustering Exploration (K-Means) ---\")\n\n# --- Configuration ---\nOUTPUT_DIR = '/kaggle/working/output'\nRESULTS_FILE = os.path.join(OUTPUT_DIR, 'analysis_results.pkl')\n\n# --- Load Previous Results ---\nanalysis_results = None\nif os.path.exists(RESULTS_FILE):\n    try:\n        with open(RESULTS_FILE, 'rb') as f:\n            analysis_results = pickle.load(f)\n        print(f\"Loaded previous results from {RESULTS_FILE}\")\n    except Exception as e:\n        print(f\"ERROR: Failed to load results file '{RESULTS_FILE}': {e}. Cannot proceed.\")\n        # exit() or handle\nelse:\n    print(f\"ERROR: Results file '{RESULTS_FILE}' not found. Run previous cells first.\")\n    # exit() or handle\n\n# --- Check if DataFrame 'df' exists ---\nif 'df' not in locals() or df is None:\n     print(\"ERROR: DataFrame 'df' not found in memory. Please re-run Cell 0 (Setup).\")\n     # exit() or handle\nelif analysis_results: # Proceed only if results were loaded\n\n    # --- Initialize keys ---\n    analysis_results['clustering_skipped'] = True\n    analysis_results.setdefault('clustering_results', {}) # Ensure nested dict exists\n    analysis_results['plot_paths'].setdefault('clustering_pca', None) # Specific key for plot\n\n    clustering_features = ['Age', 'Fare', 'SibSp', 'Parch'] # Features to use\n    n_clusters_fixed = 4 # Fixed number of clusters for this example\n    analysis_results['clustering_results']['n_clusters'] = n_clusters_fixed\n    analysis_results['clustering_results']['features_used'] = clustering_features # Store features used\n\n    try: # Wrap main logic\n        # Select data and check validity\n        available_features = [f for f in clustering_features if f in df.columns]\n        if len(available_features) < 2:\n            print(\"Skipping clustering: Not enough specified features exist in the DataFrame.\")\n            analysis_results['clustering_reason'] = \"Not Applicable: Insufficient features.\"\n        else:\n            df_cluster_orig = df[available_features].copy()\n            # Check if data is purely NaN before imputing/fitting\n            if not df_cluster_orig.empty and df_cluster_orig.notna().any().any():\n                print(f\"Using features for clustering: {available_features}\")\n\n                # --- Preprocessing ---\n                imputer_cl = SimpleImputer(strategy='median')\n                df_cluster_imputed = imputer_cl.fit_transform(df_cluster_orig)\n                df_cluster_imputed_df = pd.DataFrame(df_cluster_imputed, columns=available_features, index=df_cluster_orig.index)\n                print(\" - Imputed missing values for clustering using median.\")\n\n                scaler = StandardScaler()\n                df_cluster_scaled = scaler.fit_transform(df_cluster_imputed_df)\n                print(\" - Scaled features using StandardScaler.\")\n\n                # --- Run K-Means ---\n                kmeans = KMeans(n_clusters=n_clusters_fixed, n_init='auto', random_state=42)\n                cluster_labels = kmeans.fit_predict(df_cluster_scaled)\n                analysis_results['clustering_results']['labels'] = cluster_labels\n                df_cluster_imputed_df['Cluster'] = cluster_labels # Add labels for analysis\n                print(f\" - K-Means clustering performed with K={n_clusters_fixed}.\")\n\n                # --- Analyze Clusters ---\n                cluster_summary = df_cluster_imputed_df.groupby('Cluster')[available_features].mean().round(2)\n                analysis_results['clustering_results']['summary_stats'] = cluster_summary\n                print(\" - Calculated cluster characteristics (mean values):\")\n                print(cluster_summary)\n\n                # --- Visualize Clusters (PCA) ---\n                try:\n                     pca = PCA(n_components=2, random_state=42)\n                     df_cluster_pca = pca.fit_transform(df_cluster_scaled)\n                     print(f\" - PCA performed for visualization. Explained variance ratio: {pca.explained_variance_ratio_}\")\n\n                     fig, ax = plt.subplots(figsize=(10, 7))\n                     sns.scatterplot(x=df_cluster_pca[:, 0], y=df_cluster_pca[:, 1], hue=cluster_labels, palette='viridis', s=50, alpha=0.7, ax=ax)\n                     ax.set_title(f'Data Points Clustered into {n_clusters_fixed} Groups (PCA)')\n                     ax.set_xlabel('Principal Component 1')\n                     ax.set_ylabel('Principal Component 2')\n                     ax.legend(title='Cluster')\n                     ax.grid(True, linestyle='--', alpha=0.5)\n                     fig.tight_layout()\n                     plot_filename = os.path.join(OUTPUT_DIR, 'clustering_pca_scatter.png')\n                     fig.savefig(plot_filename)\n                     plt.close(fig)\n\n                     if os.path.exists(plot_filename):\n                          analysis_results['plot_paths']['clustering_pca'] = plot_filename\n                          print(\" - Saved cluster PCA scatter plot.\")\n                     else: print(\" - FAILED to save cluster PCA scatter plot.\")\n                except Exception as viz_e:\n                     print(f\" - ERROR during clustering visualization: {viz_e}\")\n\n                analysis_results['clustering_skipped'] = False\n                print(\"Clustering exploration successful.\")\n                # Clean up intermediate objects\n                del df_cluster_scaled, df_cluster_pca, cluster_labels, df_cluster_imputed_df\n            else:\n                print(\"Skipping clustering: No suitable features or only NaN values after selection.\")\n                analysis_results['clustering_reason'] = \"Not Applicable: Insufficient data after cleaning.\"\n\n    except Exception as e:\n        print(f\"ERROR during Clustering Exploration processing: {e}\")\n\n    # Clean up original copy\n    if 'df_cluster_orig' in locals(): del df_cluster_orig\n    gc.collect()\n\n    # --- Save Updated Results ---\n    if analysis_results:\n        try:\n            with open(RESULTS_FILE, 'wb') as f:\n                pickle.dump(analysis_results, f)\n            print(f\"Clustering results saved to {RESULTS_FILE}\")\n            print(\"\\n--- Clustering Exploration Complete ---\")\n            print(\"Proceed to next cell (Time Series Check / LLM Generation).\")\n        except Exception as e:\n            print(f\"ERROR: Failed to save clustering results: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T17:28:05.044015Z","iopub.execute_input":"2025-04-11T17:28:05.044441Z","iopub.status.idle":"2025-04-11T17:28:05.739003Z","shell.execute_reply.started":"2025-04-11T17:28:05.044402Z","shell.execute_reply":"2025-04-11T17:28:05.737776Z"}},"outputs":[{"name":"stdout","text":"--- Running Clustering Exploration (K-Means) ---\nLoaded previous results from /kaggle/working/output/analysis_results.pkl\nUsing features for clustering: ['Age', 'Fare', 'SibSp', 'Parch']\n - Imputed missing values for clustering using median.\n - Scaled features using StandardScaler.\n - K-Means clustering performed with K=4.\n - Calculated cluster characteristics (mean values):\n           Age    Fare  SibSp  Parch\nCluster                             \n0        24.38   17.28   0.27   0.24\n1        47.24   74.59   0.47   0.25\n2        18.32   34.65   4.36   1.55\n3        37.38  189.80   1.08   3.92\n - PCA performed for visualization. Explained variance ratio: [0.37757793 0.30875452]\n - Saved cluster PCA scatter plot.\nClustering exploration successful.\nClustering results saved to /kaggle/working/output/analysis_results.pkl\n\n--- Clustering Exploration Complete ---\nProceed to next cell (Time Series Check / LLM Generation).\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Cell 6: Time Series Check & Analysis\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport pickle\nimport gc\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n# Note: Assumes necessary libraries (pandas, numpy, statsmodels, matplotlib) are loaded\n\nprint(\"--- Running Time Series Check & Analysis ---\")\n\n# --- Configuration ---\nOUTPUT_DIR = '/kaggle/working/output'\nRESULTS_FILE = os.path.join(OUTPUT_DIR, 'analysis_results.pkl')\n\n# --- Load Previous Results ---\nanalysis_results = None\nif os.path.exists(RESULTS_FILE):\n    try:\n        with open(RESULTS_FILE, 'rb') as f:\n            analysis_results = pickle.load(f)\n        print(f\"Loaded previous results from {RESULTS_FILE}\")\n    except Exception as e:\n        print(f\"ERROR: Failed to load results file '{RESULTS_FILE}': {e}. Cannot proceed.\")\n        # exit() or handle\nelse:\n    print(f\"ERROR: Results file '{RESULTS_FILE}' not found. Run previous cells first.\")\n    # exit() or handle\n\n# --- Check if DataFrame 'df' exists ---\nif 'df' not in locals() or df is None:\n     print(\"ERROR: DataFrame 'df' not found in memory. Please re-run Cell 0 (Setup).\")\n     # exit() or handle\nelif analysis_results: # Proceed only if results were loaded\n\n    # --- Redefine Helper/Analysis Functions needed in this cell ---\n    # Redefining here makes the cell more self-contained if run independently later\n    def find_time_column(df):\n        \"\"\"Attempts to find a likely time/date/sequence column.\"\"\"\n        print(\"\\n--- Checking for Time Series Column ---\")\n        for col in df.select_dtypes(include=['datetime64']).columns: print(f\" - Found datetime column: {col}\"); return col\n        time_keywords = ['date', 'time', 'timestamp', 'period', 'year', 'month', 'day', 'dt']\n        for col in df.columns:\n            if any(keyword in col.lower() for keyword in time_keywords):\n                try:\n                    if pd.to_datetime(df[col], errors='coerce').notna().mean() > 0.5: print(f\" - Found likely time column by name: {col}\"); return col\n                except Exception: continue\n        if isinstance(df.index, pd.DatetimeIndex) and not isinstance(df.index, pd.RangeIndex): print(f\" - Found DatetimeIndex. Consider resetting index.\"); return None\n        print(\" - No obvious time/date column found.\"); return None\n\n    def run_time_analysis(df_original, time_col_name, results, out_dir):\n        \"\"\"Performs basic time series analysis if a time column is found.\"\"\"\n        print(f\"\\n--- Running Time Series Analysis on column: {time_col_name} ---\")\n        results['time_analysis_skipped'] = True; results['plot_paths'].setdefault('time_series', {}); results['narratives'].setdefault('time_series_description',\"Skipped/Failed.\"); results['narratives'].setdefault('time_series_suggestions',\"N/A.\"); results['time_analysis_reason']=\"Analysis started.\"\n        target_col = None;\n        try:\n            df_ts = df_original.copy(); # Use df_ts locally\n            try:\n                df_ts[time_col_name]=pd.to_datetime(df_ts[time_col_name], errors='coerce'); dropped=len(df_ts); df_ts.dropna(subset=[time_col_name], inplace=True); dropped=dropped-len(df_ts);\n                if dropped > 0: print(f\" - Warn: Dropped {dropped} rows with bad dates.\");\n                if len(df_ts) < 12: print(f\" - Skip: Not enough valid time points ({len(df_ts)}).\"); results['time_analysis_reason'] = \"NA: Insufficient data.\"; return results\n                df_ts=df_ts.sort_values(by=time_col_name).set_index(time_col_name); print(f\" - Set {time_col_name} as index.\")\n            except Exception as e: print(f\"ERROR processing time column {time_col_name}: {e}\"); results['time_analysis_reason'] = f\"Failed: Error processing column.\"; return results\n\n            numeric_cols=df_ts.select_dtypes(include=np.number).columns; cols_exclude=['PassengerId','Survived','Pclass'];\n            for col in numeric_cols:\n                 if col not in cols_exclude: target_col=col; break\n            if target_col is None: print(\"Skip TS: No suitable numerical column.\"); results['time_analysis_reason'] = \"NA: No numerical column.\"; return results\n\n            print(f\" - Analyzing '{target_col}' over time.\"); ts_data=df_ts[target_col].dropna()\n            if len(ts_data) < 12: print(f\"Skip plots: Not enough data points ({len(ts_data)}) in '{target_col}'.\"); results['time_analysis_reason'] = f\"NA: Insufficient data in {target_col}.\"; return results\n\n            # Trend Plot\n            try:\n                fig, ax=plt.subplots(figsize=(12, 5)); ts_data.plot(ax=ax, label=target_col, title=f'Trend of {target_col}');\n                try: \n                    window=max(1, min(len(ts_data)//10, 50));\n                    if len(ts_data)>=window: ts_data.rolling(window=window).mean().plot(ax=ax, label=f'Rolling Mean ({window})', linestyle='--')\n                except: pass # Ignore rolling mean error\n                ax.legend(); ax.grid(True, alpha=0.5); fig.tight_layout(); fname=os.path.join(out_dir, f'trend_{target_col}.png'); fig.savefig(fname); plt.close(fig);\n                if os.path.exists(fname): results['plot_paths']['time_series'][f'trend_{target_col}'] = fname; print(\" - Saved trend plot.\")\n            except Exception as e: print(f\" - Error trend plot {target_col}: {e}\")\n\n            # Decomposition\n            try:\n                period = 12; # Example period\n                if len(ts_data) >= 2*period:\n                    print(f\" - Decomposing (period={period})...\"); decomp = seasonal_decompose(ts_data, model='additive', period=period, extrapolate_trend='freq'); fig_d = decomp.plot(); fig_d.set_size_inches(10, 8); plt.suptitle(f\"Decomposition {target_col}\", y=1.02); plt.tight_layout(rect=[0,0.03,1,0.98]); fname_d=os.path.join(out_dir, f'decomp_{target_col}.png'); fig_d.savefig(fname_d); plt.close(fig_d);\n                    if os.path.exists(fname_d): results['plot_paths']['time_series'][f'decomp_{target_col}'] = fname_d; print(\" - Saved decomp plot.\")\n                    results['time_analysis_results'] = {'target_col': target_col, 'period_used': period} # Store results\n                else: print(f\" - Skip decomp: Need {2*period} points, have {len(ts_data)}.\")\n            except Exception as e: print(f\" - Error decomposition {target_col}: {e}\")\n\n            results['time_analysis_skipped']=False; results['time_analysis_reason']=\"Analysis performed.\"; print(\"Time series analysis successful.\")\n        except Exception as e: print(f\"ERROR during Time Series Analysis Section: {e}\"); results['time_analysis_reason']=f\"Failed: {type(e).__name__}.\"\n        gc.collect(); return results\n\n\n    # --- Main Logic for this Cell ---\n    try:\n        # Find time column\n        time_col = find_time_column(df) # Pass the DataFrame loaded in Cell 0\n\n        if time_col:\n            # Run the analysis function if column found\n            analysis_results = run_time_analysis(df, time_col, analysis_results, OUTPUT_DIR)\n        else:\n            # Update skipped reason if no column found\n            analysis_results['time_analysis_skipped'] = True\n            analysis_results['time_analysis_reason'] = \"Not Applicable: No suitable time/sequence column identified.\"\n            print(\"Time series analysis skipped as no suitable column was found.\")\n\n        # --- Save Updated Results ---\n        try:\n            with open(RESULTS_FILE, 'wb') as f:\n                pickle.dump(analysis_results, f)\n            print(f\"Time series check results saved to {RESULTS_FILE}\")\n            print(\"\\n--- Time Series Check / Analysis Complete ---\")\n            print(\"Proceed to next cell (LLM Generation).\")\n        except Exception as e:\n            print(f\"ERROR: Failed to save time series check results: {e}\")\n\n    except Exception as e:\n        print(f\"ERROR during Time Series Check processing: {e}\")\n\nelse:\n     print(\"Cannot run Time Series Check: analysis_results failed to load or df is missing.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T17:30:49.239465Z","iopub.execute_input":"2025-04-11T17:30:49.239847Z","iopub.status.idle":"2025-04-11T17:30:49.515690Z","shell.execute_reply.started":"2025-04-11T17:30:49.239820Z","shell.execute_reply":"2025-04-11T17:30:49.514375Z"}},"outputs":[{"name":"stdout","text":"--- Running Time Series Check & Analysis ---\nLoaded previous results from /kaggle/working/output/analysis_results.pkl\n\n--- Checking for Time Series Column ---\n - No obvious time/date column found.\nTime series analysis skipped as no suitable column was found.\nTime series check results saved to /kaggle/working/output/analysis_results.pkl\n\n--- Time Series Check / Analysis Complete ---\nProceed to next cell (LLM Generation).\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Cell 7: LLM Narrative & Suggestion Generation\n\nimport pandas as pd\nimport numpy as np\nimport os\nimport pickle\nimport gc\nimport google.generativeai as genai\n# Note: Other imports might be needed if running stand-alone, but assume df exists\n\nprint(\"--- Running LLM Narrative & Suggestion Generation ---\")\n\n# --- Configuration ---\nOUTPUT_DIR = '/kaggle/working/output'\nRESULTS_FILE = os.path.join(OUTPUT_DIR, 'analysis_results.pkl')\n\n# --- Load Previous Results ---\nanalysis_results = None\nif os.path.exists(RESULTS_FILE):\n    try:\n        with open(RESULTS_FILE, 'rb') as f:\n            analysis_results = pickle.load(f)\n        print(f\"Loaded previous results from {RESULTS_FILE}\")\n    except Exception as e:\n        print(f\"ERROR: Failed to load results file '{RESULTS_FILE}': {e}. Cannot proceed.\")\n        # exit() or handle\nelse:\n    print(f\"ERROR: Results file '{RESULTS_FILE}' not found. Run previous cells first.\")\n    # exit() or handle\n\n# --- Check if DataFrame 'df' exists ---\nif 'df' not in locals() or df is None:\n     print(\"ERROR: DataFrame 'df' not found in memory. Please re-run Cell 0 (Setup).\")\n     # exit() or handle\nelif analysis_results: # Proceed only if results were loaded\n\n    gemini_ready = analysis_results.get('gemini_configured', False)\n    print(f\"Gemini Readiness Check: {gemini_ready}\")\n\n    # --- Redefine Helper Function ---\n    def get_gemini_narrative(prompt, gemini_ready, model_name=\"models/gemini-1.5-flash-latest\"):\n        \"\"\"Generates narrative using Gemini, handles errors.\"\"\"\n        if not gemini_ready: return \"LLM processing skipped: Not configured.\"\n        try:\n            model = genai.GenerativeModel(model_name); response=model.generate_content(prompt, generation_config=genai.types.GenerationConfig(temperature=0.7), request_options={'timeout': 120});\n            if response.candidates and hasattr(response.candidates[0], 'content') and hasattr(response.candidates[0].content, 'parts') and len(response.candidates[0].content.parts) > 0:\n                 generated_text = response.candidates[0].content.parts[0].text; return generated_text.strip() if generated_text else \"Narrative generation failed (empty text).\"\n            else: feedback = getattr(response, 'prompt_feedback', 'N/A'); finish_reason = getattr(response.candidates[0], 'finish_reason', 'N/A') if response.candidates else 'N/A'; print(f\"WARN: LLM response structure issue. Finish Reason: {finish_reason}. Feedback: {feedback}\"); return \"Narrative generation failed (unexpected response).\"\n        except Exception as e: print(f\"ERROR during Gemini API call ({model_name}): {e}\"); return f\"Narrative generation failed due to API error: {type(e).__name__}\"\n\n    # --- Redefine LLM Generation Function (with internal try-except) ---\n    def run_llm_generation(df, results, gemini_ready):\n        print(\"\\n--- Running LLM Narrative & Suggestion Generation ---\")\n        if not gemini_ready: print(\"Skipping ALL LLM Generation.\"); narr_keys=['overview','statistics','nuances','visualizations','correlation','anomaly_description','clustering_description','domain_insights','time_series_description']; sugg_keys=['nuances_suggestions','viz_suggestions','corr_suggestions','clustering_suggestions','risk_identification','time_series_suggestions']; [results['narratives'].setdefault(key, \"LLM Skipped.\") for key in narr_keys+sugg_keys]; return results\n\n        print(\"Generating main narratives & suggestions...\"); results['narratives'] = {}\n        narr_keys=['overview','statistics','nuances','visualizations','correlation','anomaly_description','clustering_description','domain_insights','time_series_description']; sugg_keys=['nuances_suggestions','viz_suggestions','corr_suggestions','clustering_suggestions','risk_identification','time_series_suggestions']; [results['narratives'].setdefault(key, \"Gen Pending...\") for key in narr_keys+sugg_keys]\n\n        # --- Generate Narratives (WITH internal try-except for robustness) ---\n        print(\" - Generating Overview Narrative...\")\n        try: shape=results.get('shape','N/A'); head_str=results.get('head', pd.DataFrame()).to_string(); prompt=f\"Dataset Shape: {shape}. First 5 Rows:\\n{head_str}\\n\\nWrite 2-3 sentence intro paragraph (mention Sex, Age, Pclass).\"; results['narratives']['overview'] = get_gemini_narrative(prompt, gemini_ready)\n        except Exception as e: results['narratives']['overview'] = f\"Error: {e}\"\n\n        print(\" - Generating Statistics Narrative...\")\n        try: stats_str=results.get('summary_stats',pd.DataFrame()).to_string(); prompt=f\"Stats:\\n{stats_str}\\n\\nWrite 3-5 sentence summary. Mention Age/Fare (skew?), Sex/Pclass/Embarked (top categories).\"; results['narratives']['statistics'] = get_gemini_narrative(prompt, gemini_ready)\n        except Exception as e: results['narratives']['statistics'] = f\"Error: {e}\"\n\n        print(\" - Generating Nuances Narrative...\")\n        try:\n            missing_sr=results.get('missing_values'); total_rows=results.get('shape',(0,0))[0]; missing_str=\"None.\"; missing_summary=\"\"\n            if missing_sr is not None and not missing_sr.empty: missing_summary_list=[f\"{i} ({((v/total_rows)*100):.1f}%)\" for i,v in missing_sr.items()]; missing_summary=\", \".join(missing_summary_list); missing_str=\"\\n\".join([f\"- {i}: {v} ({((v/total_rows)*100):.1f}%)\" for i,v in missing_sr.items()])\n            unique_sr=results.get('unique_counts',pd.Series()); unique_str=unique_sr.to_string() if unique_sr is not None else \"N/A\"; prompt=f\"{total_rows} rows.\\nMissing:\\n{missing_str}\\nUnique:\\n{unique_str}\\n\\nWrite 3-5 sentence quality summary. Highlight missing (Cabin, Age), identifiers (PassengerId), categoricals (Pclass, Sex).\"; results['narratives']['nuances']=get_gemini_narrative(prompt, gemini_ready)\n        except Exception as e: results['narratives']['nuances']=f\"Error: {e}\"\n\n        print(\" - Generating Visualizations Narrative...\")\n        try:\n            skew_data=results.get('numerical_skewness',{}); skew_str=\", \".join([f\"{k}: {v:.2f}\" for k,v in skew_data.items()]); hist_keys=list(results.get('plot_paths',{}).get('histograms',{})); count_keys=list(results.get('plot_paths',{}).get('count_plots',{}))\n            prompt=f\"Histograms: {hist_keys}. Skew: {skew_str}. Count plots: {count_keys}. Write 3-5 sentence summary of distribution shapes (skew) & dominant categories.\"\n            results['narratives']['visualizations'] = get_gemini_narrative(prompt, gemini_ready)\n        except Exception as e: results['narratives']['visualizations'] = f\"Error: {e}\"\n\n        print(\" - Generating Correlation Narrative...\")\n        try:\n            corr_pairs=results.get('highly_correlated_pairs',[]); narrative_corr=\"Skipped/Failed.\"\n            if not results.get('correlation_skipped') and corr_pairs: corr_str=\", \".join([f\"{p[0]} ({p[1]:.2f})\" for p in corr_pairs]); prompt=f\"Notable correlations (abs>0.4): {corr_str}. Write 2-4 sentence explanation (e.g., Pclass/Fare).\"; narrative_corr=get_gemini_narrative(prompt, gemini_ready)\n            elif not results.get('correlation_skipped'): narrative_corr=\"No significant correlations (>0.4) identified.\"\n            else: narrative_corr=\"Correlation analysis skipped.\"\n            results['narratives']['correlation'] = narrative_corr\n        except Exception as e: results['narratives']['correlation'] = f\"Error: {e}\"\n\n        print(\" - Generating Anomaly Description Narrative...\")\n        try:\n            anomaly_features = results.get('anomaly_results',{}).get('features_used',['N/A'])\n            if not results.get('anomaly_skipped'): anomaly_count=results['anomaly_results']['count']; anomaly_perc=results['anomaly_results']['percentage']; prompt=f\"Isolation Forest found {anomaly_count} ({anomaly_perc:.2f}%) potential anomalies using {anomaly_features}. Explain briefly what this means (1-2 sentences).\"; results['narratives']['anomaly_description'] = get_gemini_narrative(prompt, gemini_ready)\n            else: results['narratives']['anomaly_description'] = \"Anomaly detection skipped.\"\n        except Exception as e: results['narratives']['anomaly_description'] = f\"Error: {e}\"\n\n        print(\" - Generating Clustering Description Narrative...\")\n        try:\n            clustering_features = results.get('clustering_results',{}).get('features_used',['N/A'])\n            if not results.get('clustering_skipped'): k=results['clustering_results']['n_clusters']; cluster_summary_str=results['clustering_results']['summary_stats'].to_string(); prompt=f\"K={k} clusters found using {clustering_features}. Cluster Means:\\n{cluster_summary_str}\\n\\nWrite paragraph describing key characteristics differentiating these {k} clusters based on means (3-5 sentences).\"; results['narratives']['clustering_description'] = get_gemini_narrative(prompt, gemini_ready)\n            else: results['narratives']['clustering_description'] = \"Clustering analysis skipped.\"\n        except Exception as e: results['narratives']['clustering_description'] = f\"Error: {e}\"\n\n        print(\" - Generating Domain Insights Narrative...\")\n        try: column_names = df.columns.tolist(); dtypes_str = df.dtypes.to_string(); prompt=f\"Based ONLY on columns: {column_names} & types:\\n{dtypes_str}\\nWhat is the likely domain (e.g., travel, health)? What are 1-2 typical analysis goals or advanced analyses? Be brief (2-3 sentences) & state this is speculative.\"; results['narratives']['domain_insights'] = get_gemini_narrative(prompt, gemini_ready)\n        except Exception as e: results['narratives']['domain_insights'] = f\"Error: {e}\"\n\n        print(\" - Generating Time Series Narrative...\")\n        try:\n            if not results.get('time_analysis_skipped'):\n                ts_plots=list(results.get('plot_paths',{}).get('time_series',{})); ts_target=results.get('time_analysis_results',{}).get('target_col','N/A'); ts_period=results.get('time_analysis_results',{}).get('period_used','N/A')\n                context=f\"Time series analysis performed on '{ts_target}'. Plots generated: {ts_plots}. Decomposition assumed period={ts_period}.\"\n                prompt=f\"Context: [{context}]. Write brief summary of analysis & observed trends/patterns.\"\n                results['narratives']['time_series_description'] = get_gemini_narrative(prompt, gemini_ready)\n            else: results['narratives']['time_series_description'] = results.get('time_analysis_reason', \"TS analysis skipped/NA.\")\n        except Exception as e: results['narratives']['time_series_description'] = f\"Error: {e}\"\n\n        # --- Generate Suggestions & Risk ID ---\n        print(\"\\nGenerating suggestions & risk ID...\")\n        try: prompt=f\"Findings: [Missing data: {missing_summary}. Identifiers: PassengerId, Name. Categoricals: Survived, Pclass, Sex, Embarked.]. Implications/next steps for analysis/modeling? (1-2 points)\"; results['narratives']['nuances_suggestions'] = get_gemini_narrative(prompt, gemini_ready)\n        except Exception as e: results['narratives']['nuances_suggestions'] = f\"Error: {e}\"\n        try: prompt=f\"Findings: [Skewness: {skew_str}. Dominant categories in: {count_keys}.]. Implications of distributions/dominance for feature engineering/modeling? (1-2 points)\"; results['narratives']['viz_suggestions'] = get_gemini_narrative(prompt, gemini_ready)\n        except Exception as e: results['narratives']['viz_suggestions'] = f\"Error: {e}\"\n        try:\n             if not results.get('correlation_skipped') and 'corr_str' in locals() and corr_str: context=f\"Correlations: {corr_str}.\"; prompt=f\"Findings: [{context}]. What might these signify? Next steps for exploration?\"; results['narratives']['corr_suggestions'] = get_gemini_narrative(prompt, gemini_ready)\n             else: results['narratives']['corr_suggestions'] = \"N/A.\"\n        except Exception as e: results['narratives']['corr_suggestions'] = f\"Error: {e}\"\n        try:\n            if not results.get('clustering_skipped'): cluster_summary_str=results['clustering_results']['summary_stats'].to_string(); k=results['clustering_results']['n_clusters']; prompt=f\"Findings: [{k} clusters found with characteristics:\\n{cluster_summary_str}]. What might these segments represent (e.g., Titanic context)? Potential next steps?\"; results['narratives']['clustering_suggestions'] = get_gemini_narrative(prompt, gemini_ready)\n            else: results['narratives']['clustering_suggestions'] = \"N/A.\"\n        except Exception as e: results['narratives']['clustering_suggestions'] = f\"Error: {e}\"\n        try:\n            if not results.get('time_analysis_skipped'): ts_plots=list(results.get('plot_paths',{}).get('time_series',{})); ts_target=results.get('time_analysis_results',{}).get('target_col','N/A'); context=f\"Time series analysis on '{ts_target}'. Plots: {ts_plots}.\"; prompt=f\"Context: [{context}]. Based on visual trends/decomposition, suggest implications or next steps for forecasting/analysis?\"; results['narratives']['time_series_suggestions'] = get_gemini_narrative(prompt, gemini_ready)\n            else: results['narratives']['time_series_suggestions'] = \"N/A.\"\n        except Exception as e: results['narratives']['time_series_suggestions'] = f\"Error: {e}\"\n        try: skew_risk=[k for k,v in skew_data.items() if abs(v)>2]; anomaly_perc=results.get('anomaly_results',{}).get('percentage',0); context=f\"Missing Data: {missing_summary}. Highly Skewed: {skew_risk}. Potential Anomalies: {anomaly_perc:.1f}%. High Cardinality (noise risk): Ticket.\"; results['narratives']['risk_identification'] = get_gemini_narrative(f\"Synthesize data quality risks: [{context}]. Write brief paragraph on main risks (bias from missing data, impact of skew/outliers/anomalies, noisy features).\", gemini_ready); print(\"Generated Risk Identification narrative.\")\n        except Exception as e: results['narratives']['risk_identification'] = f\"Error: {e}\"\n\n        # Print summary\n        print(\"\\nGenerated Narratives & Suggestions Summary:\"); [print(f\" - {k}: {'Generated' if isinstance(v,str) and len(v)>10 and 'N/A' not in v and 'Error' not in v and 'failed' not in v.lower() and 'skipped' not in v.lower() else v[:60]+'...'}\") for k,v in results.get('narratives',{}).items()];\n        return results # Return updated results\n\n    # --- Main logic for this cell ---\n    try:\n        # Run the LLM generation function\n        analysis_results = run_llm_generation(df, analysis_results, gemini_ready)\n\n        # --- Save Final Results (including narratives) ---\n        try:\n            with open(RESULTS_FILE, 'wb') as f:\n                pickle.dump(analysis_results, f)\n            print(f\"\\nFinal results with narratives saved to {RESULTS_FILE}\")\n            print(\"\\n--- LLM Generation Complete ---\")\n            print(\"Proceed to next cell (HTML Generation).\")\n        except Exception as e:\n            print(f\"ERROR: Failed to save final results after LLM generation: {e}\")\n\n    except Exception as e:\n        print(f\"ERROR during LLM Generation processing: {e}\")\nelse:\n     print(\"Cannot run LLM Generation: analysis_results failed to load or df is missing.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T17:32:05.910421Z","iopub.execute_input":"2025-04-11T17:32:05.911326Z","iopub.status.idle":"2025-04-11T17:32:27.079835Z","shell.execute_reply.started":"2025-04-11T17:32:05.911290Z","shell.execute_reply":"2025-04-11T17:32:27.078724Z"}},"outputs":[{"name":"stdout","text":"--- Running LLM Narrative & Suggestion Generation ---\nLoaded previous results from /kaggle/working/output/analysis_results.pkl\nGemini Readiness Check: True\n\n--- Running LLM Narrative & Suggestion Generation ---\nGenerating main narratives & suggestions...\n - Generating Overview Narrative...\n - Generating Statistics Narrative...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"name":"stdout","text":" - Generating Nuances Narrative...\n - Generating Visualizations Narrative...\n - Generating Correlation Narrative...\n - Generating Anomaly Description Narrative...\n - Generating Clustering Description Narrative...\n - Generating Domain Insights Narrative...\n - Generating Time Series Narrative...\n\nGenerating suggestions & risk ID...\nGenerated Risk Identification narrative.\n\nGenerated Narratives & Suggestions Summary:\n - overview: Generated\n - statistics: Generated\n - nuances: Generated\n - visualizations: Generated\n - correlation: Generated\n - anomaly_description: Generated\n - clustering_description: Generated\n - domain_insights: Generated\n - time_series_description: Generated\n - nuances_suggestions: Generated\n - viz_suggestions: Generated\n - corr_suggestions: Generated\n - clustering_suggestions: Generated\n - risk_identification: Generated\n - time_series_suggestions: N/A....\n\nFinal results with narratives saved to /kaggle/working/output/analysis_results.pkl\n\n--- LLM Generation Complete ---\nProceed to next cell (HTML Generation).\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# Cell 8: HTML & PDF Report Generation (Select Table Columns & Add Note)\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt # Still potentially needed by helpers indirectly\nimport seaborn as sns # Still potentially needed by helpers indirectly\nimport os\nimport io\nimport base64\nimport pickle\nimport gc\n# WeasyPrint check needed for completeness\ntry: from weasyprint import HTML as WPHTML, CSS; WEASYPRINT_AVAILABLE = True\nexcept ImportError: WEASYPRINT_AVAILABLE = False # Set flag based on availability\nfrom IPython.display import HTML, display, FileLink\n\nprint(\"--- Running Final Report Generation (Select Table Columns) ---\")\n\n# --- Configuration ---\nOUTPUT_DIR = '/kaggle/working/output'\nRESULTS_FILE = os.path.join(OUTPUT_DIR, 'analysis_results.pkl')\nHTML_REPORT_PATH = os.path.join(OUTPUT_DIR, 'analysis_report_selected_cols.html') # New HTML name\nPDF_REPORT_PATH = os.path.join(OUTPUT_DIR, 'analysis_report_selected_cols.pdf')  # New PDF name\n\n# --- Load Final Results ---\nanalysis_results = None\nif os.path.exists(RESULTS_FILE):\n    try:\n        with open(RESULTS_FILE, 'rb') as f:\n            analysis_results = pickle.load(f)\n        print(f\"Loaded final results from {RESULTS_FILE}\")\n    except Exception as e:\n        print(f\"ERROR: Failed to load results file '{RESULTS_FILE}': {e}. Cannot proceed.\")\n        analysis_results = None # Ensure it's None if load fails\nelse:\n    print(f\"ERROR: Results file '{RESULTS_FILE}' not found. Run previous cells first.\")\n    analysis_results = None # Ensure it's None if file not found\n\n# --- Check if results loaded ---\nif analysis_results:\n\n    # --- Redefine HTML Generation Function and Helpers ---\n    def df_to_html(df_in):\n        # Helper to convert DataFrame to HTML\n        if df_in is None: return \"<p>N/A</p>\"\n        try:\n            # Make a copy to avoid modifying the original DataFrame in analysis_results\n            df_display = df_in.copy()\n            # Convert any non-string columns needed for display back to string if necessary\n            # (e.g., if dtypes were changed) - usually to_html handles this.\n            return df_display.to_html(escape=False, na_rep='-', justify='left', classes='dataframe', border=1)\n        except Exception as e: print(f\"WARN: df_to_html failed: {e}\"); return \"<p>Table display error</p>\"\n\n    def format_missing_html(missing_series, total_rows):\n        # Helper to format missing values list\n        if missing_series is None or missing_series.empty: return \"<p>No missing values detected.</p>\"\n        lines=[\"<ul>\"]+[f\"<li><b>{i}:</b> {v} ({((v/total_rows)*100):.1f}%)</li>\" for i,v in missing_series.items()]+[\"</ul>\"]; return \"\\n\".join(lines)\n\n    def format_plot_html(plot_path, alt_text=\"Plot\"):\n        # Helper to embed plots using Base64\n        if plot_path and os.path.exists(plot_path):\n            try:\n                with open(plot_path, \"rb\") as f: encoded = base64.b64encode(f.read()).decode('utf-8')\n                fmt = os.path.splitext(plot_path)[1][1:].lower().replace('jpg','jpeg')\n                return f'<img src=\"data:image/{fmt};base64,{encoded}\" alt=\"{alt_text}\" style=\"max-width:80%; width: 100%; height:auto;display:block;margin:10px auto;border:1px solid #eee;\">'\n            except Exception as e: print(f\"WARN: Base64 encoding failed for {plot_path}: {e}\"); return f\"<p><i>ERROR encoding plot: {os.path.basename(plot_path)}</i></p>\"\n        else: return f\"<p><i>Plot file not found: {os.path.basename(plot_path) if plot_path else alt_text}.</i></p>\"\n\n    # --- generate_html_report MODIFIED ---\n    def generate_html_report(results, out_dir, report_path):\n        print(\"\\n--- Running Dynamic HTML Report Generation Logic (Selected Columns) ---\")\n        # Define which columns are generally useful for direct display in tables\n        # Omit 'Name', 'Ticket', 'Cabin' as they are wide or mostly empty\n        COLS_TO_DISPLAY_IN_TABLE = ['PassengerId', 'Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n\n        css_styles=results.get('css_styles', \"\"\"\n            body{font-family:sans-serif;margin:20px;line-height:1.5;background-color:#fdfdfd;}\n            h1{color:#2c3e50;border-bottom:2px solid #3498db;padding-bottom:10px;}\n            h2{color:#3498db;border-bottom:1px solid #eee;padding-bottom:5px;margin-top:35px;}\n            h3{color:#555;margin-top:25px;font-size:1.1em;} h4{color:#777;margin-top:20px;font-size:1.0em;font-style:italic;}\n            p{margin-bottom:10px;}\n            /* Make table scrollable if needed, but prioritize fitting */\n            div.table-container { max-width: 100%; overflow-x: auto; margin: 20px 0; }\n            table.dataframe{border-collapse:collapse; width: auto; /* Let table size itself */ margin: 0; /* Remove margin from table itself */ font-size:0.8em; border:1px solid #ccc;}\n            table.dataframe th, table.dataframe td{border:1px solid #ddd; padding: 5px 7px; text-align: left; vertical-align: top; white-space: nowrap; /* Prevent wrapping initially */}\n            /* Allow specific problematic columns like Name (if included) to wrap */\n            /* table.dataframe td:nth-child(4), table.dataframe th:nth-child(4) { white-space: normal; word-wrap: break-word; } */\n            table.dataframe th{background-color:#f0f5f9;font-weight:bold;} table.dataframe tbody tr:nth-child(even){background-color:#f9f9f9;}\n            img{max-width:85%;height:auto;display:block;margin:20px auto;border:1px solid #ddd;padding:3px;background-color:white;}\n            .suggestion{background-color:#f0f8ff;border-left:5px solid #79bdee;padding:10px 15px;margin:20px 0;font-style:normal;}\n            .suggestion p{margin:5px 0;} ul{margin-left:20px;padding-left:10px;} li{margin-bottom:6px;}\n            section{margin-bottom:30px;padding-bottom:20px;border-bottom:1px dashed #ccc;} section:last-of-type{border-bottom:none;}\n            .omitted-note { font-size: 0.8em; color: #666; margin-top: 5px; }\n        \"\"\") # Added table-container and omitted-note styles\n        narratives=results.get('narratives', {});\n\n        html_parts=[f\"\"\"<!DOCTYPE html><html lang=\"en\"><head><meta charset=\"UTF-8\"><title>Automated Analysis Report</title><style>{css_styles}</style></head><body>\n                     <h1>Automated Analysis Report</h1><p>Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\"\"\"]\n        section_counter=1\n        def add_section(title, content_html):\n            nonlocal section_counter; html_parts.append(f\"<section><h2>{section_counter}. {title}</h2>\"); html_parts.append(content_html); html_parts.append(\"</section>\"); section_counter += 1\n\n        # --- Define function to handle table display and omitted columns note ---\n        def create_table_display_html(df_full, df_dtypes=None):\n            \"\"\" Creates HTML for a table, displaying subset of columns and noting omitted ones.\"\"\"\n            if df_full is None: return \"<p>N/A</p>\"\n\n            # Find which columns from COLS_TO_DISPLAY_IN_TABLE exist in the df\n            cols_to_show = [col for col in COLS_TO_DISPLAY_IN_TABLE if col in df_full.columns]\n            df_display = df_full[cols_to_show]\n\n            table_html = df_to_html(df_display)\n            omitted_cols = [col for col in df_full.columns if col not in cols_to_show]\n            note_html = \"\"\n            if omitted_cols:\n                 omitted_info = []\n                 if df_dtypes is not None: # Get types if available\n                      for col in omitted_cols:\n                           omitted_info.append(f\"{col} [{df_dtypes.get(col, 'N/A')}]\")\n                 else: # Otherwise just list names\n                      omitted_info = omitted_cols\n                 note_html = f\"<p class='omitted-note'><i>(Columns not displayed: {', '.join(omitted_info)})</i></p>\"\n\n            # Wrap table in a scrollable div\n            return f\"<div class='table-container'>{table_html}</div>{note_html}\"\n\n        # Get original dtypes for reference\n        original_dtypes = df.dtypes if 'df' in locals() and df is not None else None\n\n        # --- Define sections structure ---\n        sections_info=[\n            ('Overview', lambda r: True, lambda r, n: f\"<p>{n.get('overview','N/A')}</p><h3>Head (First {len(r.get('head',[]))} Rows):</h3>{create_table_display_html(r.get('head'), original_dtypes)}<h3>Tail (Last {len(r.get('tail',[]))} Rows):</h3>{create_table_display_html(r.get('tail'), original_dtypes)}\"),\n            ('Statistics', lambda r: True, lambda r, n: f\"<p>{n.get('statistics','N/A')}</p><h3>Summary Statistics:</h3>{create_table_display_html(r.get('summary_stats'))}\"), # Keep describe() full for now, might fit\n            ('Data Nuances', lambda r: True, lambda r, n: f\"<p>{n.get('nuances','N/A')}</p><h3>Missing Values:</h3>{format_missing_html(r.get('missing_values'), r.get('shape',(0,0))[0])}<h4>Potential Implications & Next Steps:</h4><div class='suggestion'><p>{n.get('nuances_suggestions','N/A')}</p></div>\"),\n            ('Basic Visualizations', lambda r: True, lambda r, n: (lambda hp, cp: f\"<p>{n.get('visualizations','N/A')}</p>\" + (f\"<h3>Histograms:</h3>{''.join([format_plot_html(p,f'Hist {c}') for c,p in hp.items()])}\" if hp else \"<p><i>No histograms generated/found.</i></p>\") + (f\"<h3>Count Plots:</h3>{''.join([format_plot_html(p,f'Count {c}') for c,p in cp.items()])}\" if cp else \"<p><i>No count plots generated/found.</i></p>\") + f\"<h4>Potential Implications & Considerations:</h4><div class='suggestion'><p>{n.get('viz_suggestions','N/A')}</p></div>\")(r.get('plot_paths',{}).get('histograms',{}), r.get('plot_paths',{}).get('count_plots',{}))),\n            ('Correlation Analysis', 'correlation_skipped', lambda r, n: (lambda cp: f\"<p>{n.get('correlation','N/A')}</p><h3>Correlation Heatmap:</h3>{format_plot_html(r.get('plot_paths',{}).get('heatmap'),'Corr Heatmap')}<h3>Highly Correlated Pairs (> 0.4):</h3>{'<ul>'+''.join([f'<li><b>{p[0]}</b>: {p[1]:.3f}</li>' for p in cp])+'</ul>' if cp else '<p>None above threshold (0.4).</p>'}<h4>Potential Implications & Further Exploration:</h4><div class='suggestion'><p>{n.get('corr_suggestions','N/A')}</p></div>\")(r.get('highly_correlated_pairs',[]))),\n            ('Anomaly Detection', 'anomaly_skipped', lambda r, n: f\"<p>{n.get('anomaly_description','N/A')}</p><ul><li>Features Analyzed: {r.get('anomaly_results',{}).get('features_used',['N/A'])}</li><li>Potential Anomalies Found: {r.get('anomaly_results',{}).get('count','N/A')} ({r.get('anomaly_results',{}).get('percentage','N/A'):.2f}%)</li></ul>\"),\n            ('Clustering Exploration', 'clustering_skipped', lambda r, n: f\"<p>{n.get('clustering_description','N/A')}</p><h3>Cluster Characteristics (K={r.get('clustering_results',{}).get('n_clusters','N/A')}):</h3>{df_to_html(r.get('clustering_results',{}).get('summary_stats'))}<h3>Cluster Visualization (PCA):</h3>{format_plot_html(r.get('plot_paths',{}).get('clustering_pca'),'Cluster PCA')}<h4>Interpretation & Next Steps:</h4><div class='suggestion'><p>{n.get('clustering_suggestions','N/A')}</p></div>\"), # Cluster summary table might also be wide\n            ('Risk Identification Summary', None, lambda r, n: f\"<div class='suggestion'><p>{n.get('risk_identification','N/A')}</p></div>\"),\n            ('Domain Insights (Speculative)', None, lambda r, n: f\"<div class='suggestion'><p>{n.get('domain_insights','N/A')}</p></div>\"),\n            ('Time Series Analysis', 'time_analysis_skipped', lambda r, n: (f\"<p>{n.get('time_series_description','N/A')}</p>\" + (lambda tps: (\"<h3>Plots:</h3>\"+(''.join([format_plot_html(p, f'{k} Plot') for k,p in tps.items()]))) if tps else \"\")(r.get('plot_paths',{}).get('time_series',{})) + f\"<h4>Implications & Further Analysis:</h4><div class='suggestion'><p>{n.get('time_series_suggestions','N/A')}</p></div>\")), # Removed extra parenthesis again\n            (\"Interactive Visualizations\", None, lambda r,n: \"<i>Module not yet implemented.</i>\"),\n            (\"What-If Scenarios\", None, lambda r,n: \"<i>Analysis skipped (requires specific modeling choices).</i>\"),\n            (\"Multi-Dataset Comparisons\", None, lambda r,n: \"<i>Analysis skipped (requires multiple input datasets).</i>\"),\n            (\"Trend Analysis\", None, lambda r,n: \"<i>Module not yet implemented (often part of Time Series).</i>\"),\n        ]\n\n        # Loop through defined sections and add if not skipped\n        for title, skip_key, content_func in sections_info:\n            is_skipped = results.get(skip_key, False) if skip_key else False\n            if skip_key and is_skipped: # If a skip key exists and is True\n                 reason = results.get(f\"{skip_key.replace('_skipped','')}_reason\", \"Analysis skipped or not applicable.\")\n                 add_section(title, f\"<p><i>{reason}</i></p>\")\n            else: # Otherwise, generate the content\n                 add_section(title, content_func(results, narratives))\n\n        html_parts.append(\"</body></html>\"); html_content = \"\\n\".join(html_parts)\n\n        # Write HTML File\n        try:\n            with open(report_path, 'w', encoding='utf-8') as f: f.write(html_content)\n            print(f\"\\nFinal HTML report generated: {report_path}\\nLink: {os.path.join('/kaggle/working/output', os.path.basename(report_path))}\")\n        except Exception as e: print(f\"\\nError writing HTML report: {e}\"); html_content = None\n        print(\"\\n--- Dynamic HTML Report Generation Complete ---\");\n        return html_content # Return HTML content string\n\n    # --- Redefine PDF Generation Function ---\n    def create_pdf_from_html(html_string, pdf_path):\n        print(\"\\n--- Running PDF Generation ---\")\n        if not WEASYPRINT_AVAILABLE: print(\"Skipping PDF Generation: WeasyPrint not available.\"); return False\n        if not html_string: print(\"Skipping PDF Generation: HTML content is empty.\"); return False\n        try:\n            print(f\"Attempting to write PDF to: {pdf_path}\")\n            WPHTML(string=html_string).write_pdf(pdf_path)\n            print(f\"PDF report generated successfully: {pdf_path}\")\n            print(f\"Download Link (Kaggle):\")\n            display(FileLink(pdf_path))\n            return True\n        except Exception as e: print(f\"\\nERROR during PDF Generation: {e}\\nCheck WeasyPrint dependencies.\"); return False\n\n    # --- Main logic for this cell ---\n    html_content_output = None\n    pdf_generated_output = False\n    try:\n        # Generate HTML report content using the MODIFIED function\n        html_content_output = generate_html_report(analysis_results, OUTPUT_DIR, HTML_REPORT_PATH)\n\n        # Generate PDF report from HTML content\n        if html_content_output:\n            pdf_generated_output = create_pdf_from_html(html_content_output, PDF_REPORT_PATH)\n\n        print(\"\\n--- Report Generation Complete ---\")\n        if pdf_generated_output: print(\"Both HTML and PDF reports generated.\")\n        elif html_content_output: print(\"HTML report generated; PDF failed/skipped.\")\n        else: print(\"HTML report generation failed.\")\n\n    except Exception as e:\n        print(f\"ERROR during Report Generation processing: {e}\")\n\nelse:\n     print(\"Cannot run Report Generation: analysis_results failed to load.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T18:00:50.068129Z","iopub.execute_input":"2025-04-11T18:00:50.068588Z","iopub.status.idle":"2025-04-11T18:00:51.655140Z","shell.execute_reply.started":"2025-04-11T18:00:50.068558Z","shell.execute_reply":"2025-04-11T18:00:51.654285Z"}},"outputs":[{"name":"stdout","text":"--- Running Final Report Generation (Select Table Columns) ---\nLoaded final results from /kaggle/working/output/analysis_results.pkl\n\n--- Running Dynamic HTML Report Generation Logic (Selected Columns) ---\n\nFinal HTML report generated: /kaggle/working/output/analysis_report_selected_cols.html\nLink: /kaggle/working/output/analysis_report_selected_cols.html\n\n--- Dynamic HTML Report Generation Complete ---\n\n--- Running PDF Generation ---\nAttempting to write PDF to: /kaggle/working/output/analysis_report_selected_cols.pdf\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"name":"stdout","text":"PDF report generated successfully: /kaggle/working/output/analysis_report_selected_cols.pdf\nDownload Link (Kaggle):\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/output/analysis_report_selected_cols.pdf","text/html":"<a href='/kaggle/working/output/analysis_report_selected_cols.pdf' target='_blank'>/kaggle/working/output/analysis_report_selected_cols.pdf</a><br>"},"metadata":{}},{"name":"stdout","text":"\n--- Report Generation Complete ---\nBoth HTML and PDF reports generated.\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"from IPython.display import FileLink, display\nimport os\n\n# Define the path to your generated PDF file\npdf_report_path = '/kaggle/working/output/analysis_report_final.pdf'\n\n# Check if the file exists before creating the link\nif os.path.exists(pdf_report_path):\n  print(f\"Generating download link for: {pdf_report_path}\")\n  display(FileLink(pdf_report_path))\nelse:\n  print(f\"ERROR: PDF file not found at {pdf_report_path}\")\n  print(\"Please ensure the main analysis cell ran successfully and generated the PDF.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T17:52:45.093015Z","iopub.execute_input":"2025-04-11T17:52:45.093426Z","iopub.status.idle":"2025-04-11T17:52:45.102845Z","shell.execute_reply.started":"2025-04-11T17:52:45.093395Z","shell.execute_reply":"2025-04-11T17:52:45.101520Z"}},"outputs":[{"name":"stdout","text":"Generating download link for: /kaggle/working/output/analysis_report_final.pdf\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/output/analysis_report_final.pdf","text/html":"<a href='/kaggle/working/output/analysis_report_final.pdf' target='_blank'>/kaggle/working/output/analysis_report_final.pdf</a><br>"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}